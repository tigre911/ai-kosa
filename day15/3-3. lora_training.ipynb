{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/ostamand/midjourney-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install diffusers\n",
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "# ~/.config/kaggle.json\n",
    "!kaggle datasets download ostamand/midjourney-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo apt install unzip\n",
    "!unzip midjourney-v0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from diffusers.training_utils import compute_snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(model_name, dtype=torch.float16):\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(dtype=dtype)\n",
    "    vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(dtype=dtype)\n",
    "    scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\").to(dtype=dtype)\n",
    "    return tokenizer, text_encoder, vae, scheduler, unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidjourneyDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, tokenizer: CLIPTokenizer):\n",
    "        self.data_dir = data_dir\n",
    "        self.df = pd.read_csv(data_dir / \"index.csv\")\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.train_tranforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_dir / \"images\" / self.df.iloc[idx][\"image\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.train_tranforms(image)\n",
    "\n",
    "        input_ids = self.tokenizer(\n",
    "            self.df.iloc[idx][\"short_prompt\"],\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"][0]\n",
    "\n",
    "        return {\"pixel_values\": image, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_models_for_training(model_name, rank: int=128):\n",
    "    tokenizer, text_encoder, vae, scheduler, unet = get_models(model_name)\n",
    "\n",
    "    # freeze all weights\n",
    "    for m in (unet, text_encoder, vae):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # config LoRA\n",
    "    unet_lora_config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=rank,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    )\n",
    "\n",
    "    unet.add_adapter(unet_lora_config)\n",
    "\n",
    "    # set trainaible weights to float32\n",
    "    for p in unet.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.to(dtype=torch.float32)\n",
    "\n",
    "    return tokenizer, text_encoder, vae, scheduler, unet\n",
    "\n",
    "def get_lora_params(unet):\n",
    "    return [p for p in filter(lambda p: p.requires_grad, [p for p in unet.parameters()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(BaseModel):\n",
    "    train_steps: int = 100\n",
    "    lr: float = 1e-5\n",
    "    batch_size: int = 4\n",
    "    accumulation_steps: int = 2\n",
    "    rank: int = 128\n",
    "    max_grad_norm: float = 1.0\n",
    "    pretrained_name: str = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "    data_dir: str = \"./midjourney-512px-v0\"\n",
    "    snr_gamma: float = -1\n",
    "    seed: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    tokenizer: CLIPTokenizer, \n",
    "    text_encoder: CLIPTextModel, \n",
    "    vae: AutoencoderKL, \n",
    "    scheduler: DDPMScheduler, \n",
    "    unet: UNet2DConditionModel,\n",
    "    config: TrainingConfig,\n",
    "    device = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    lora_params = get_lora_params(unet)\n",
    "\n",
    "    text_encoder.to(device).eval()\n",
    "    vae.to(device).eval()\n",
    "    unet.to(device).train()\n",
    "\n",
    "    # data set\n",
    "    train_dataset = MidjourneyDataset(Path(config.data_dir), tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    # optimizer\n",
    "    steps_per_epoch = math.ceil(len(train_dataloader) / config.accumulation_steps)\n",
    "    epochs = math.ceil(config.train_steps / steps_per_epoch)\n",
    "\n",
    "    lr = config.lr * config.accumulation_steps * config.batch_size\n",
    "    optimizer = torch.optim.AdamW(lora_params, lr=lr)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # progress bar setup\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(\n",
    "        range(config.train_steps),\n",
    "        desc=\"Steps\"\n",
    "    )\n",
    "\n",
    "    print(f\"configs: {config}\")\n",
    "    print(f\"epochs: {epochs}\")\n",
    "    print(f\"steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"total steps: {config.train_steps}\")\n",
    "    print(f\"accumulation steps: {config.accumulation_steps}\")\n",
    "    print(f\"total batch size: {config.batch_size * config.accumulation_steps}\")\n",
    "    print(f\"lr: {lr}\")\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            bs = batch[\"input_ids\"].shape[0]\n",
    "\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(device), return_dict=False)[0]\n",
    "\n",
    "                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bs,)).long().to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"pixel_values\"].to(device)).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                noise = torch.randn_like(latents)\n",
    "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
    "\n",
    "                if config.snr_gamma > 0:\n",
    "                    # should converge faster with snr_gamma, however works well with unweighted mse\n",
    "                    # https://arxiv.org/abs/2303.09556\n",
    "                    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py\n",
    "                    snr = compute_snr(scheduler, timesteps)\n",
    "                    mse_loss_weights = torch.stack([snr, config.snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
    "                        dim=1\n",
    "                    )[0]\n",
    "                    mse_loss_weights = mse_loss_weights / snr\n",
    "                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
    "                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "                    loss = loss.mean()\n",
    "                else:\n",
    "                    loss = F.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "            global_step+=1\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if global_step % config.accumulation_steps == 0:\n",
    "                if config.max_grad_norm > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(lora_params, config.max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": losses[-1]})\n",
    "            if global_step / config.accumulation_steps >= config.train_steps:\n",
    "                break\n",
    "    \n",
    "    return {\n",
    "        \"losses\": losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # in case of rerun, to make sure we free up GPU before calling train\n",
    "    del models, pipe\n",
    "    import gc; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "config = TrainingConfig()\n",
    "config.lr = 1e-5\n",
    "config.rank = 62\n",
    "config.train_steps = 1000\n",
    "config.snr_gamma = 5.0\n",
    "config.seed = 42\n",
    "\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "models = setup_models_for_training(config.pretrained_name, rank=config.rank)\n",
    "\n",
    "outputs = train(\n",
    "    *models,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outputs[\"losses\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = models[-1]\n",
    "unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StableDiffusionPipeline.save_lora_weights(\n",
    "    save_directory=\"./out\",\n",
    "    unet_lora_layers=unet_lora_state_dict,\n",
    "    safe_serialization=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l -h ./out/pytorch_lora_weights.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(config.data_dir) / \"index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prompt_training = [df.iloc[0][\"short_prompt\"]]\n",
    "prompt_new = \"a dark forest, detailed, 8k\"\n",
    "num_inference_steps=35\n",
    "seed = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(pipeline, prompt, seed):\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    return pipeline(prompt, num_inference_steps=num_inference_steps, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    config.pretrained_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(Path(config.data_dir) / \"images\" / df.iloc[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(pipe, prompt_training, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(pipe, prompt_new, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_lora_weights(\"./out/pytorch_lora_weights.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(pipe, prompt_training, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(pipe, prompt_new, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
